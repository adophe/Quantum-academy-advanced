
import { GoogleGenAI, Type } from "@google/genai";
import { ExplanationResponse, UserMode, InteractionMode, AppSettings } from "../types";

export class AdaptiveAcademyService {
  /**
   * Generates high-fidelity scientific diagrams using Gemini 2.5 Flash Image.
   * gemini-2.5-flash-image is used to avoid the mandatory user API key selection UI requirement.
   */
  async generateVisual(prompt: string): Promise<string | null> {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    try {
      const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: [{ 
          parts: [{ 
            text: `A professional, high-fidelity scientific diagram or mathematical graph for an elite academy. 
                   Subject: ${prompt}. 
                   Style: Dark mode, neon cyan and violet color palette, white labels, crisp lines, technical and precise.` 
          }] 
        }],
        config: {
          imageConfig: {
            aspectRatio: "16:9"
          }
        }
      });

      for (const part of response.candidates?.[0]?.content?.parts || []) {
        if (part.inlineData) {
          return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
      return null;
    } catch (error) {
      console.error("Visual Generation Error:", error);
      return null;
    }
  }

  /**
   * Processes the pedagogical flow using Gemini 3 Flash reasoning for speed and general compatibility.
   */
  async processPedagogy(
    input: string,
    history: { role: 'user' | 'model'; text: string }[],
    settings: AppSettings,
    problemImageB64?: string,
    userFaceImageB64?: string
  ): Promise<ExplanationResponse> {
    const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
    const model = 'gemini-3-flash-preview';
    
    const systemInstruction = `
      ### CORE INSTRUCTIONS: QUANTUM ACADEMY NEURAL TUTOR ###
      You are an elite AI Teaching Assistant, NOT a casual chatbot.
      
      CURRENT_MODE: ${settings.interactionMode.toUpperCase()}
      LEARNER: ${settings.userName} (${settings.userMode} mode)
      SUBJECT: ${settings.activeSubject} | TOPIC: ${settings.activeTopic}
      FORMAT: ${settings.preferredFormat} | SESSION: ${settings.sessionLength}

      --- TEACHING MODE PROTOCOL (STRICT) ---
      1. PEDAGOGICAL STRUCTURE: Break content into very small sections (chunks).
      2. VERIFICATION LOOP: After every section/chunk, you MUST ask a simple question to verify understanding. 
         WAIT for the learner's response before continuing to the next topic.
      3. NO OVERLOAD: Use simple language, analogies, and concrete examples.
      4. CAMERA AWARENESS: If [SYSTEM_INPUT: BIOMETRIC_SCAN] indicates confusion or distraction, 
         you MUST slow down, re-explain using a simpler method, and ask: "Should I explain this in another way?".
      5. ACCESSIBILITY: 
         - Sight Problem: Provide extremely descriptive text for TTS. Avoid visual references like "as you can see".
         - Hearing Problem: Prioritize clear text, symbols, and bold formatting. Use the visualPrompt field for diagrams.
      6. TIME CONTROL: Respect the ${settings.sessionLength} limit.

      --- RESPONSE SCHEMA (STRICT JSON) ---
      {
        "subject": "Clear Topic Header",
        "assessment": "Diagnostic of learner's cognitive state.",
        "explanation": "Linear, chunked lesson content (Use KaTeX for math: $...$ or $$...$$).",
        "visualPrompt": "A technical prompt for a visual diagram.",
        "followUp": "The MANDATORY verification question."
      }
    `;

    const contents: any[] = history.map(h => ({
      role: h.role,
      parts: [{ text: h.text }]
    }));

    const currentParts: any[] = [];
    if (problemImageB64) {
      currentParts.push({ inlineData: { mimeType: 'image/jpeg', data: problemImageB64.split(',')[1] || problemImageB64 } });
    }
    if (userFaceImageB64) {
      currentParts.push({ text: "[SYSTEM_INPUT: BIOMETRIC_SCAN]" });
      currentParts.push({ inlineData: { mimeType: 'image/jpeg', data: userFaceImageB64.split(',')[1] || userFaceImageB64 } });
    }
    
    currentParts.push({ text: input || "Initialize pedagogical session." });
    contents.push({ role: 'user', parts: currentParts });

    try {
      const response = await ai.models.generateContent({
        model,
        contents,
        config: {
          systemInstruction,
          responseMimeType: "application/json",
          responseSchema: {
            type: Type.OBJECT,
            properties: {
              subject: { type: Type.STRING },
              assessment: { type: Type.STRING },
              explanation: { type: Type.STRING },
              visualPrompt: { type: Type.STRING },
              followUp: { type: Type.STRING }
            },
            required: ["subject", "assessment", "explanation", "followUp"]
          },
          tools: settings.advancedMode ? [{ googleSearch: {} }] : undefined
        }
      });

      const parsed = JSON.parse(response.text || '{}');
      const chunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks;
      const links = chunks?.filter(c => c.web).map(c => ({ 
        title: c.web?.title || 'Scientific Resource', 
        uri: c.web?.uri || '' 
      })) || [];

      return { ...parsed, groundingLinks: links };
    } catch (error) {
      throw error;
    }
  }
}
